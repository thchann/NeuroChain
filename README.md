# NeuroChain  
*A learning journey from perceptrons to GPT-style transformers*  

## ğŸ“Œ Overview  
Core2GPT is an **educational machine learning framework** that bridges classical algorithms and modern deep learning. Itâ€™s designed to help you implement, train, and understand models ranging from **perceptrons and regression** all the way to **transformer-based GPT models** â€” using real datasets like **MNIST** and a custom **language ID task**.  

This project provides a **hands-on path** to see how simple ideas in machine learning grow into the foundations of todayâ€™s large language models.  

## ğŸ› ï¸ Features  
- **Classical Models**: Perceptron, Regression, Digit Classification  
- **Deep Learning with PyTorch**: Fully connected networks and training pipelines  
- **Transformers & GPT**: Character-level GPT (`chargpt.py`) and GPT-like transformer (`gpt_model.py`)  
- **Datasets**:  
  - `mnist.npz` â†’ handwritten digit recognition  
  - `lang_id.npz` â†’ language identification  
- **Testing Tools**: `autograder.py` for automated checks  

## ğŸ“‚ Project Structure
```
â”‚â”€â”€ backend.py # Utilities and data handling
â”‚â”€â”€ models.py # Core ML models (Perceptron, Regression, etc.)
â”‚â”€â”€ gpt_model.py # Transformer-based GPT model
â”‚â”€â”€ chargpt.py # Character-level GPT implementation
â”‚â”€â”€ autograder.py # Automated grading/testing
â”‚â”€â”€ data/
â”‚ â”œâ”€â”€ mnist.npz # Digit classification dataset
â”‚ â””â”€â”€ lang_id.npz # Language ID dataset
â”‚â”€â”€ input.txt # Example input for GPT
```

## ğŸš€ Installation  
```
Clone the repository and install dependencies:  
```bash
git clone https://github.com/<your-username>/Core2GPT.git
cd Core2GPT
pip install torch numpy
```

## ğŸƒ Running tests
```
python autograder.py
```